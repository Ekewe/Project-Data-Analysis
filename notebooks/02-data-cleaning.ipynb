{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eceb1a01-2d65-4c04-a655-1ad16a516012",
   "metadata": {},
   "source": [
    "# Data Cleaning for Complaint Descriptions\n",
    "\n",
    "In this notebook, we will clean and preprocess the complaint descriptions extracted from the dataset. The goal of this step is to transform the raw complaint text into a standardized format suitable for NLP analysis. By the end of this notebook, the text data will be ready for topic modeling and other NLP techniques to identify prevalent themes.\n",
    "\n",
    "## Steps in this Notebook\n",
    "1. **Load the Complaint Data**: Load the complaint descriptions saved from the previous notebook.\n",
    "2. **Text Preprocessing**: Apply various cleaning techniques to standardize the text, including:\n",
    "   - Lowercasing text\n",
    "   - Removing punctuation, numbers, and special characters\n",
    "   - Removing common stopwords\n",
    "   - Applying stemming or lemmatization\n",
    "   - Checking for Duplicate Complaints\n",
    "3. **Final Output**: Save the cleaned complaint descriptions for use in subsequent analysis.\n",
    "\n",
    "Each of these steps will help us focus on the meaningful content of each complaint, making it easier to identify recurring topics and patterns across the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ef55a-2ac1-46f0-ad98-c2729be8968b",
   "metadata": {},
   "source": [
    "### Lowercasing Text\n",
    "To standardize the complaint descriptions, we will convert all text to lowercase. This ensures consistency and helps avoid treating words with different cases (e.g., \"Building\" vs. \"building\") as separate entities in later analysis steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97f9accd-815b-4966-a490-6abb8135c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c6cb25f-050d-431d-9b21-d0d5554e2a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Complaint Text\n",
      "0          painting base of building without permits\n",
      "1        work being done; working on building façade\n",
      "2  construction of rear yard addition without per...\n",
      "3                               painting front walls\n",
      "4                       installation of bar on patio\n"
     ]
    }
   ],
   "source": [
    "# Load the complaints data\n",
    "complaints_df = pd.read_csv('../data/processed/complaints_extracted.csv')\n",
    "\n",
    "# Convert all complaint text to lowercase\n",
    "complaints_df['Complaint Text'] = complaints_df['Complaint Text'].str.lower()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(complaints_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a8602-2b45-4f49-a7c4-c050febcc6f1",
   "metadata": {},
   "source": [
    "### Removing Punctuation, Numbers, and Special Characters\n",
    "In this step, we will remove punctuation, numbers, and any special characters from the complaint descriptions. This helps to focus on the meaningful words in each complaint, making the text cleaner and easier to analyze in later stages. By eliminating these extra elements, we avoid unnecessary noise that could interfere with identifying relevant topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796e00d5-e6f0-406e-9e5c-404c543bb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Complaint Text\n",
      "0          painting base of building without permits\n",
      "1         work being done working on building façade\n",
      "2  construction of rear yard addition without per...\n",
      "3                               painting front walls\n",
      "4                       installation of bar on patio\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation, numbers, and special characters\n",
    "complaints_df['Complaint Text'] = complaints_df['Complaint Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\sà-ÿÀ-ß]', '', x))\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(complaints_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3e500-382e-4cb0-a366-20f82e4c99e2",
   "metadata": {},
   "source": [
    "### Normalizing Accented Characters\n",
    "\n",
    "During the cleaning process, we realized that some words, such as \"façade,\" contain accented characters that are important to the word’s meaning. To prevent the loss of these important words, we will normalize all accented characters to their closest ASCII equivalents. For example, \"façade\" will become \"facade.\" This step ensures consistency across the dataset while retaining the readability of the words, which is essential for effective analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50bb3f73-1678-479a-910e-4bfbca1c9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Apply the normalization\n",
    "complaints_df['Complaint Text'] = complaints_df['Complaint Text'].apply(remove_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59046730-3676-499f-a7fc-b1ffb7f3e76a",
   "metadata": {},
   "source": [
    "### Removing Common Stopwords\n",
    "Next, we’ll remove common stopwords from the complaint descriptions. Stopwords are frequently used words that don’t add significant meaning to the text, such as \"the,\" \"and,\" \"is,\" etc. Removing these words helps us focus on the core content of each complaint and reduces noise in the data, which is especially helpful for topic modeling and other NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e74aabcc-ece5-4268-83d7-b38428f27377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Complaint Text\n",
      "0             painting base building without permits\n",
      "1                  work done working building facade\n",
      "2  construction rear yard addition without permit...\n",
      "3                               painting front walls\n",
      "4                             installation bar patio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "complaints_df['Complaint Text'] = complaints_df['Complaint Text'].apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in stop_words])\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(complaints_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da4d29-9597-44fa-a6b8-b78d5381c008",
   "metadata": {},
   "source": [
    "### Lemmatization with Part-of-Speech (POS) Tagging\n",
    "\n",
    "Next, we will apply lemmatization to reduce each word in the complaints to its base form. This process helps standardize the text by grouping variations of words (e.g., \"running\" and \"run\") into a single form, making it more consistent for analysis.\n",
    "\n",
    "To enhance accuracy, we will use **Part-of-Speech (POS) tagging** to identify the grammatical role of each word. By tagging each word as a noun, verb, adjective, or adverb, we can apply lemmatization in a context-aware manner, modifying words only where appropriate. For instance:\n",
    "- **Nouns** (e.g., \"buildings\") will be lemmatized to their singular form (\"building\").\n",
    "- **Verbs** (e.g., \"working\") will be reduced to their root form (\"work\").\n",
    "\n",
    "This POS-aware approach to lemmatization will help us retain the meaning and context of the words, producing cleaner and more accurate text for our subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb988f1c-7495-4461-94fb-6d6f2eb03d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Emman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Emman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Complaint Text\n",
      "0                 paint base building without permit\n",
      "1                       work do work building facade\n",
      "2  construction rear yard addition without permit...\n",
      "3                                   paint front wall\n",
      "4                             installation bar patio\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Define function to convert nltk POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization with POS tagging\n",
    "complaints_df['Complaint Text'] = complaints_df['Complaint Text'].apply(\n",
    "    lambda x: ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) \n",
    "                        for word, pos in pos_tag(x.split())])\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(complaints_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f42bdc-360e-44bd-8afb-db0faaf438ad",
   "metadata": {},
   "source": [
    "### Checking for Duplicate Complaints\n",
    "\n",
    "Before saving the cleaned data, we will check for any duplicate complaint descriptions. Removing duplicate entries ensures that each complaint is represented only once, preventing redundant data from impacting our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "802f9372-7a14-4005-8dcf-4e72a50f9044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate complaints: 1864\n",
      "Number of complaints after removing duplicates: 3668\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate entries\n",
    "duplicates = complaints_df.duplicated(subset=['Complaint Text']).sum()\n",
    "print(f\"Number of duplicate complaints: {duplicates}\")\n",
    "\n",
    "# Remove duplicates if any are found\n",
    "complaints_df = complaints_df.drop_duplicates(subset=['Complaint Text'])\n",
    "\n",
    "# Confirm the new number of entries\n",
    "print(f\"Number of complaints after removing duplicates: {len(complaints_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97415e6a-b8d1-40c8-833a-e94c2c39b4ec",
   "metadata": {},
   "source": [
    "### Saving the Cleaned Data\n",
    "\n",
    "With all preprocessing steps completed, we will now save the cleaned complaint descriptions to a new CSV file. This file will serve as the final dataset, ready for further analysis such as topic modeling or sentiment analysis in future steps. Saving the data ensures that we can easily reload it in subsequent notebooks without repeating the cleaning steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee3bfb9a-627b-4be6-9d30-0062d7f312f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned complaints data to a CSV file\n",
    "complaints_df.to_csv('../data/processed/cleaned_complaints.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
