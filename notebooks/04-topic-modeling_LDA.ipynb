{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6427fb8-6049-487c-a1d4-35b6baccc4e3",
   "metadata": {},
   "source": [
    "# Topic Modeling on Complaint Descriptions\n",
    "\n",
    "In this notebook, we will apply topic modeling to the vectorized complaint descriptions. Topic modeling is an unsupervised technique that helps identify recurring themes or topics within large text datasets. By examining these topics, we can uncover common issues and concerns within the complaints data, aiding in better decision-making.\n",
    "\n",
    "We will primarily use **Latent Dirichlet Allocation (LDA)**, a probabilistic model that represents each complaint as a mixture of topics and each topic as a mixture of words. \n",
    "\n",
    "### Goals of this Notebook\n",
    "1. **Load Vectorized Data**: Load the Bag-of-Words or TF-IDF matrices created in the previous notebook.\n",
    "2. **Apply LDA for Topic Extraction**: Use LDA to identify key topics across the complaints.\n",
    "3. **Interpret Topics**: Display the top words for each topic to understand the main themes in the data.\n",
    "4. **Evaluate Topics with Coherence Score**: Calculate a coherence score to assess the interpretability and quality of the topics.\n",
    "\n",
    "This structured approach will allow us to derive meaningful insights from the complaints data, providing a clearer understanding of prevalent issues in New York City's landmark violation reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cd142-7e83-4fcf-9b30-22263f698533",
   "metadata": {},
   "source": [
    "### Loading Vectorized Data\n",
    "\n",
    "In this section, we load the Bag-of-Words or TF-IDF vectors generated in the previous notebook. These vector representations of the complaint text data will serve as input for our topic modeling analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb352747-772e-4602-b783-123a6daf823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Bag-of-Words (BoW) Matrix and the vectorized object\n",
    "bow_vectors = scipy.sparse.load_npz('../data/processed/bow_vectors.npz')\n",
    "with open('../data/processed/vectorizer_bow.pkl', 'rb') as f:\n",
    "    vectorizer_bow = pickle.load(f)\n",
    "\n",
    "tfidf_vectors = scipy.sparse.load_npz('../data/processed/tfidf_vectors.npz')\n",
    "with open('../data/processed/vectorizer_tfidf.pkl', 'rb') as f:\n",
    "    vectorizer_tfidf = pickle.load(f)\n",
    "\n",
    "# Load the dataframe\n",
    "complaints_df = pd.read_csv('../data/processed/cleaned_complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e169148-cbe7-4839-b3b6-4ff2e2bda1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Topic Distribution (BoW): (5532, 30)\n",
      "Document-Topic Distribution (BoW): (5532, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 30\n",
    "\n",
    "# Fit LDA model on the BoW matrix and get document-topic distribution\n",
    "lda_model_bow = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_topic_distribution_bow = lda_model_bow.fit_transform(bow_vectors)\n",
    "\n",
    "# Fit LDA model on the TF-IDF matrix and get document-topic distribution\n",
    "lda_model_tfidf = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_topic_distribution_tfidf = lda_model_tfidf.fit_transform(tfidf_vectors)\n",
    "\n",
    "# Confirm fitting by checking shapes of document-topic distributions\n",
    "print(\"Document-Topic Distribution (BoW):\", lda_topic_distribution_bow.shape)\n",
    "print(\"Document-Topic Distribution (BoW):\", lda_topic_distribution_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b28ad509-5e5a-4b58-9b8d-9da293379897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Get the feature names\n",
    "feature_names_bow = vectorizer_bow.get_feature_names_out()\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Create gensim dictionary from the sklearn vectorizer vocabulary\n",
    "tokenized_text = [text.split() for text in complaints_df['Complaint Text']]\n",
    "gensim_dictionary_bow = Dictionary([feature_names_bow.tolist()])\n",
    "gensim_dictionary_tfidf = Dictionary([feature_names_tfidf.tolist()])\n",
    "\n",
    "# Create corpus for gensim coherence scoring\n",
    "corpus = [gensim_dictionary_bow.doc2bow(text) for text in tokenized_text]\n",
    "corpus = [gensim_dictionary_tfidf.doc2bow(text) for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccbf897c-b1b5-478b-b3f8-a0087c31513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for BoW: 0.43166365403793333\n",
      "Coherence Score for TF-IDF: 0.43166365403793333\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Fit LDA model with gensim\n",
    "lda_gensim_bow = LdaModel(corpus=corpus, id2word=gensim_dictionary_bow, num_topics=n_topics, random_state=42)\n",
    "lda_gensim_tfidf = LdaModel(corpus=corpus, id2word=gensim_dictionary_tfidf, num_topics=n_topics, random_state=42)\n",
    "\n",
    "# Calculate coherence score using the C_v metric\n",
    "coherence_model_bow = CoherenceModel(model=lda_gensim_bow, texts=tokenized_text, dictionary=gensim_dictionary_bow, coherence='c_v')\n",
    "coherence_score_bow = coherence_model_bow.get_coherence()\n",
    "coherence_model_tfidf = CoherenceModel(model=lda_gensim_tfidf, texts=tokenized_text, dictionary=gensim_dictionary_tfidf, coherence='c_v')\n",
    "coherence_score_tfidf = coherence_model_tfidf.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score for BoW: {coherence_score_bow}\")\n",
    "print(f\"Coherence Score for TF-IDF: {coherence_score_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d61ea02-143d-403f-8b6c-f74d87946f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best topic number: 35\n",
      "Best coherence score: 0.4412935525371132\n"
     ]
    }
   ],
   "source": [
    "# Define the range of topics to consider\n",
    "topics_range = range(2, 50)\n",
    "\n",
    "# Initialize variables to store the best topic number and coherence score\n",
    "best_topic_num = 0\n",
    "best_coherence_score = 0\n",
    "\n",
    "# Iterate over the range of topics\n",
    "for num_topics in topics_range:\n",
    "    # Fit LDA model on the BoW matrix and get document-topic distribution\n",
    "    lda_model_bow = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_topic_distribution_bow = lda_model_bow.fit_transform(bow_vectors)\n",
    "\n",
    "    # Fit LDA model on the TF-IDF matrix and get document-topic distribution\n",
    "    lda_model_tfidf = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_topic_distribution_tfidf = lda_model_tfidf.fit_transform(tfidf_vectors)\n",
    "\n",
    "    # Get the feature names\n",
    "    feature_names_bow = vectorizer_bow.get_feature_names_out()\n",
    "    feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "    # Create gensim dictionary from the sklearn vectorizer vocabulary\n",
    "    tokenized_text = [text.split() for text in complaints_df['Complaint Text']]\n",
    "    gensim_dictionary_bow = Dictionary([feature_names_bow.tolist()])\n",
    "    gensim_dictionary_tfidf = Dictionary([feature_names_tfidf.tolist()])\n",
    "\n",
    "    # Create corpus for gensim coherence scoring\n",
    "    corpus = [gensim_dictionary_bow.doc2bow(text) for text in tokenized_text]\n",
    "    corpus = [gensim_dictionary_tfidf.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    # Fit LDA model with gensim\n",
    "    lda_gensim_bow = LdaModel(corpus=corpus, id2word=gensim_dictionary_bow, num_topics=num_topics, random_state=42)\n",
    "    lda_gensim_tfidf = LdaModel(corpus=corpus, id2word=gensim_dictionary_tfidf, num_topics=num_topics, random_state=42)\n",
    "\n",
    "    # Calculate coherence score using the C_v metric\n",
    "    coherence_model_bow = CoherenceModel(model=lda_gensim_bow, texts=tokenized_text, dictionary=gensim_dictionary_bow, coherence='c_v')\n",
    "    coherence_score_bow = coherence_model_bow.get_coherence()\n",
    "    coherence_model_tfidf = CoherenceModel(model=lda_gensim_tfidf, texts=tokenized_text, dictionary=gensim_dictionary_tfidf, coherence='c_v')\n",
    "    coherence_score_tfidf = coherence_model_tfidf.get_coherence()\n",
    "\n",
    "    # Check if the current coherence score is the highest\n",
    "    if coherence_score_bow > best_coherence_score:\n",
    "        best_topic_num = num_topics\n",
    "        best_coherence_score = coherence_score_bow\n",
    "\n",
    "# Print the best topic number and coherence score\n",
    "print(f\"Best topic number: {best_topic_num}\")\n",
    "print(f\"Best coherence score: {best_coherence_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
